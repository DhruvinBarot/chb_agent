ğŸ§  Pain & Substance-Use Research AI Agent (RAG)

A domain-aware Retrieval-Augmented Generation (RAG) system that enables interactive querying of peer-reviewed literature on pain, substance use, and behavioral health.
The system retrieves evidence from uploaded PDFs, reasons over the content using LLMs, and returns citation-grounded answers through a web-based chat interface.

âœ¨ Key Features

ğŸ“„ PDF ingestion & semantic indexing (no fine-tuning required)

ğŸ” Topic-aware retrieval with reranking (Multi-Query + RRF ready)

ğŸ§  LLM reasoning grounded in retrieved evidence

ğŸ“š Automatic citations (paper + chunk level)

ğŸ’¬ Interactive web chat UI (collapsible sources)

ğŸ§± Memory management (short-term + long-term summaries)

ğŸ›¡ï¸ Safety & domain relevance gating

ğŸš€ Production-ready backend (FastAPI, Docker-friendly)



ğŸ§© High-Level Architecture

User Query
   â†“
Safety & Domain Check
   â†“
Intent Classification + Normalization
   â†“
Topic-Aware Retrieval (ChromaDB)
   â†“
Reranking (Cross-Encoder)
   â†“
LLM Reasoning (RAG Prompt)
   â†“
Answer + Citations
   â†“
Memory Update (Short + Long Term)


ğŸ“ Project Structure

.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # FastAPI entrypoint
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ chat.py              # /chat API
â”‚   â”‚   â”œâ”€â”€ status.py            # system status
â”‚   â”‚   â””â”€â”€ files.py             # PDF upload
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ retrieval.py         # Chroma retrieval + rerank
â”‚   â”‚   â”œâ”€â”€ llm_reasoning.py     # RAG answer generation
â”‚   â”‚   â”œâ”€â”€ intent.py            # intent classification
â”‚   â”‚   â”œâ”€â”€ safety.py            # safety filtering
â”‚   â”‚   â””â”€â”€ topics.py            # domain & topic logic
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ short_term.py        # session context
â”‚   â”‚   â””â”€â”€ long_term.py         # summarized history
â”‚   â”œâ”€â”€ schemas.py               # Pydantic models
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ rate_limit.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_papers.py         # PDF ingestion pipeline
â”‚   â””â”€â”€ query_test.py            # CLI retrieval testing
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ papers/                  # uploaded PDFs
â”‚   â””â”€â”€ chroma_db/               # vector store
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.html
â”‚   â””â”€â”€ chat.html                # web UI
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ styles.css
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”„ End-to-End Workflow
1ï¸âƒ£ Document Ingestion

PDFs are uploaded or placed in data/papers/

Text is extracted, cleaned, chunked

Chunks are embedded and stored in ChromaDB

python scripts/ingest_papers.py

2ï¸âƒ£ Query Processing

User submits a question via UI or API

Query is normalized and checked for domain relevance

Topic terms guide retrieval

3ï¸âƒ£ Retrieval & Reranking

Semantic search over embedded chunks

Optional cross-encoder reranking

Low-confidence retrieval is rejected gracefully

4ï¸âƒ£ LLM Reasoning (RAG)

Retrieved evidence is injected into a structured prompt

LLM generates an answer only using retrieved context

Citations are attached per chunk

5ï¸âƒ£ Memory Updates

Short-term: conversation window

Long-term: summarized interactions for continuity

6ï¸âƒ£ Response Delivery

Answer shown first

Sources collapsed & expandable

Clean, citation-backed output

ğŸ–¥ï¸ Running the App
Create Virtual Environment
python3 -m venv .venv
source .venv/bin/activate

Install Dependencies
pip install -r requirements.txt

Start Server
uvicorn app.main:app --reload


API Docs â†’ http://127.0.0.1:8000/docs

Chat UI â†’ http://127.0.0.1:8000/chat-ui

ğŸ“¦ Core Dependencies
Backend

FastAPI

Uvicorn

Pydantic v2

Retrieval & Embeddings

ChromaDB

sentence-transformers

CrossEncoder (ms-marco-MiniLM)

LLMs (pluggable)

OpenAI (GPT-4 / GPT-4o)

HuggingFace Inference API

Local models (Ollama / vLLM supported)

Frontend

HTML / CSS

Vanilla JavaScript

Jinja2 templates

ğŸ§ª Testing
API Test
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"thread_id":"test1","message":"Summarize painâ€“opioid misuse mechanisms"}'

Retrieval Sanity Check
python scripts/query_test.py

ğŸ› ï¸ Adding New Papers

Upload PDFs or place them in data/papers/

Re-run ingestion:

python scripts/ingest_papers.py


âš ï¸ No retraining required â€” only re-embedding.

ğŸ§  Why RAG (No Fine-Tuning)?

Faster iteration

Lower cost

Full transparency

Always grounded in source documents
